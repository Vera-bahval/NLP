{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0nD0rkR8ZYf",
        "outputId": "79c31da5-8025-462d-f8b4-b4f5b8b70fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from google.colab import files\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "train_data, train_labels, test_data, valid_data, valid_labels = [], [], [], [], []\n",
        "with open ('/content/gdrive/My Drive/Машобуч ДЗ1/train_texts_2', 'r') as f1:\n",
        "  for line in f1:\n",
        "    train_data.append(line.strip())\n",
        "with open('/content/gdrive/My Drive/Машобуч ДЗ1/test_texts_2', 'r') as f2:\n",
        "  for line in f2:\n",
        "    test_data.append(line.strip())\n",
        "with open('/content/gdrive/My Drive/Машобуч ДЗ1/train_labels_2', 'r') as f3:\n",
        "  for line in f3:\n",
        "    train_labels.append(line.strip())\n",
        "with open('/content/gdrive/My Drive/Машобуч ДЗ1/train_texts', 'r') as f4:\n",
        "  for line in f4:\n",
        "    valid_data.append(line.strip())\n",
        "with open('/content/gdrive/My Drive/Машобуч ДЗ1/train_labels', 'r') as f4:\n",
        "  for line in f4:\n",
        "    valid_labels.append(line.strip())"
      ],
      "metadata": {
        "id": "9HcP55_d8xOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[lookups]"
      ],
      "metadata": {
        "id": "6tXn-vtg5LsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b081df41-e26e-4372-e9ab-3454de9264ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy[lookups] in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.3.0)\n",
            "Collecting spacy-lookups-data<1.1.0,>=1.0.3 (from spacy[lookups])\n",
            "  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[lookups]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy[lookups]) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy[lookups]) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy[lookups]) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy[lookups]) (2.1.3)\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "model = spacy.blank(\"en\")\n",
        "model.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\n",
        "model.initialize()"
      ],
      "metadata": {
        "id": "ilV5STDp6Ij1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcf7651-c6d1-4ff7-ebf1-3477803411d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thinc.optimizers.Optimizer at 0x7bb620c314e0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from string import punctuation as string_punctuation\n",
        "\n",
        "def process_text(text, lowercase=False, remove_punctuation=False, normalize_digits=False):\n",
        "    if isinstance(text, str):\n",
        "        tokens = wordpunct_tokenize(text)\n",
        "    else:\n",
        "        tokens = text\n",
        "    if lowercase:\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "    if remove_punctuation:\n",
        "        tokens = [token for token in tokens\n",
        "                  if any(x not in string_punctuation for x in token)]\n",
        "    if normalize_digits:\n",
        "        tokens = [\"<DIGIT>\" if token.isdigit() else token for token in tokens]\n",
        "    return Counter(tokens)"
      ],
      "metadata": {
        "id": "zkX7d9jw9RB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "class TextCounter:\n",
        "    \"\"\"\n",
        "    Преобразует текст в словарь (индекс слова-счётчик), убирая редкие слова\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_count=3, max_count=None, lemmatizer=None,\n",
        "                 lowercase=False, remove_punctuation=False,\n",
        "                 normalize_digits=False):\n",
        "        self.min_count = min_count\n",
        "        self.max_count = max_count\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.normalize_digits = normalize_digits\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_) if hasattr(self, \"words_\") else 0\n",
        "\n",
        "    def fit(self, texts):\n",
        "        if self.max_count is None:\n",
        "            max_count = len(texts)\n",
        "        elif self.max_count < 1:\n",
        "            max_count = len(texts) * self.max_count # доля -> абсолютное значение\n",
        "        else:\n",
        "            max_count = self.max_count\n",
        "        # обучаем словарь, собирая статистику по корпусу текстов\n",
        "        word_counts = defaultdict(int)\n",
        "        words_in_texts = dict()\n",
        "        for i, text in enumerate(texts):\n",
        "            text_word_counts = process_text(\n",
        "                text, lowercase=self.lowercase,\n",
        "                remove_punctuation=self.remove_punctuation,\n",
        "                normalize_digits=self.normalize_digits)\n",
        "            for word, count in text_word_counts.items():\n",
        "                word_counts[word] += 1\n",
        "        for word in word_counts:\n",
        "          if word not in words_in_texts:\n",
        "            words_in_texts = [word_counts[word]]\n",
        "          else:\n",
        "            words_in_texts.append(word_counts[word])\n",
        "        words_uniform = dict()\n",
        "        for word in words_in_texts:\n",
        "          data = words_in_texts[word]\n",
        "          stats, p_value = stats.kstest(data, 'uniform')\n",
        "          if p_value < 0.05:\n",
        "            words_uniform[word] = False\n",
        "          else:\n",
        "            words_uniform[word] = True\n",
        "        self.words_ = [word for word, count in word_counts.items()\n",
        "                       if count >= self.min_count and count <= max_count and not words_uniform[word]]\n",
        "        self.word_indexes_ = {word: index for index, word in enumerate(self.words_)}\n",
        "        self.word_counts_ = {\n",
        "            index: word_counts[word] for index, word in enumerate(self.words_)}\n",
        "        return self\n",
        "\n",
        "    def transform(self, text, is_processed=False):\n",
        "        word_counts = process_text(text, lowercase=self.lowercase,\n",
        "                                   remove_punctuation=self.remove_punctuation,\n",
        "                                   normalize_digits=self.normalize_digits)\n",
        "        return {self.word_indexes_[word]:count for word, count in word_counts.items()\n",
        "                if word in self.word_indexes_}"
      ],
      "metadata": {
        "id": "0MEy8AXf-O6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "data = [1, 2, 3, 4, 1, 2, 3, 5, 6, 3, 2, 3, 5, 6, 9, 10, 19, 13, 12]\n",
        "#stat, p_value = stats.kstest(data, 'uniform')\n",
        "print(stats.kstest(data, 'uniform'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHNQEzrQZUL5",
        "outputId": "c8ca477f-4706-497c-9986-bd6bb1683341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KstestResult(statistic=1.0, pvalue=0.0, statistic_location=1, statistic_sign=-1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class SparseTextVectorizer:\n",
        "\n",
        "    def __init__(self, min_count=3, max_count=None,\n",
        "                 sublinear_tf=False, use_idf=False,\n",
        "                 lemmatizer=None, lowercase=False, remove_punctuation=False,\n",
        "                 normalize_digits=False):\n",
        "        self.min_count = min_count\n",
        "        self.max_count = max_count\n",
        "        self.sublinear_tf = sublinear_tf\n",
        "        self.use_idf = use_idf\n",
        "        self.lemmatizer = lemmatizer\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.normalize_digits = normalize_digits\n",
        "\n",
        "    def fit(self, texts):\n",
        "        if self.lemmatizer is not None:\n",
        "            texts = [[token.lemma_ for token in sent]\n",
        "                     for sent in self.lemmatizer.pipe(texts)]\n",
        "        self.word_indexer_ = TextCounter(self.min_count, self.max_count,\n",
        "                                         lemmatizer=self.lemmatizer, lowercase=self.lowercase,\n",
        "                                         remove_punctuation=self.remove_punctuation,\n",
        "                                         normalize_digits=self.normalize_digits)\n",
        "        self.word_indexer_.fit(texts)\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts):\n",
        "        # нужно преобразовать тексты в разреженную матрицу\n",
        "        # A[i, j] = count <=> count(texts[j], self.words_[i]) = count > 0\n",
        "        # X = np.zeros(shape=(len(texts), len(self.word_indexer_)), dtype=float)\n",
        "        if self.lemmatizer is not None:\n",
        "            texts = [[token.lemma_ for token in sent]\n",
        "                     for sent in self.lemmatizer.pipe(texts)]\n",
        "        rows, cols, values = [], [], []\n",
        "        for i, text in enumerate(texts):\n",
        "            word_counts = self.word_indexer_.transform(text)\n",
        "            for index, count in word_counts.items():\n",
        "                # X[i, index] = count\n",
        "                rows.append(i)\n",
        "                cols.append(index)\n",
        "                if self.sublinear_tf:\n",
        "                    count = np.log2(1.0+count)\n",
        "                if self.use_idf:\n",
        "                    count /= np.log2(1.0+self.word_indexer_.word_counts_.get(index, 1))\n",
        "                values.append(count)\n",
        "        return csr_matrix((values, (rows, cols)),\n",
        "                          shape=(len(texts), len(self.word_indexer_)))"
      ],
      "metadata": {
        "id": "w_2Or88y-h_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# lowercase+remove_punctuation 78.07\n",
        "# lowercase+remove_punctuation+normalize_digits 73.01\n",
        "# lowercase+remove_punctuation min_count=3 max_count=0.1 79.54\n",
        "# lowercase+remove_punctuation min_count=3 max_count=0.1 sublinear_tf 80.63\n",
        "# lowercase+remove_punctuation min_count=3 max_count=0.1 sublinear_tf,use_idf 82.06\n",
        "# lemmatizer+lowercase+remove_punctuation min_count=3 max_count=0.1 sublinear_tf,use_idf 83.81\n",
        "vectorizer = SparseTextVectorizer(\n",
        "    lemmatizer=model, lowercase=True, remove_punctuation=True,\n",
        "    normalize_digits=False, sublinear_tf=True, use_idf=True,\n",
        "    min_count=3, max_count=0.1).fit(train_data)\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_valid = vectorizer.transform(valid_data)\n",
        "#print(type(X_train), X_train.shape, X_test.shape)\n",
        "\n",
        "cls = LogisticRegression()\n",
        "cls.fit(X_train, train_labels)\n",
        "pred_labels = cls.predict(X_valid)\n",
        "print(\" \".join(\n",
        "    f\"{100*x:.2f}\" for x in f1_score(valid_labels, pred_labels, average=None)\n",
        "))\n",
        "print(f\"{100*f1_score(valid_labels, pred_labels, average='macro'):.2f}\")"
      ],
      "metadata": {
        "id": "Bx3IPyaS_FlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test_lables.txt', 'w', encoding='utf-8') as f:\n",
        "  for x in pred_labels:\n",
        "    f.write(x+'\\n')\n",
        "files.download('test_lables.txt')"
      ],
      "metadata": {
        "id": "SD4KimMn3w2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2-NiRRS26Nl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}